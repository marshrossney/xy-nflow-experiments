{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c920ea-cef1-458b-a53f-1c7ceaf10b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:blib2to3.pgen2.driver:Generating grammar tables from /home/joe/.miniconda3/envs/xy/lib/python3.9/site-packages/blib2to3/Grammar.txt\n",
      "INFO:blib2to3.pgen2.driver:Writing grammar tables to /home/joe/.cache/black/21.12b0/Grammar3.9.7.final.0.pickle\n",
      "INFO:blib2to3.pgen2.driver:Writing failed: [Errno 2] No such file or directory: '/home/joe/.cache/black/21.12b0/tmp0pv4oyfk'\n",
      "INFO:blib2to3.pgen2.driver:Generating grammar tables from /home/joe/.miniconda3/envs/xy/lib/python3.9/site-packages/blib2to3/PatternGrammar.txt\n",
      "INFO:blib2to3.pgen2.driver:Writing grammar tables to /home/joe/.cache/black/21.12b0/PatternGrammar3.9.7.final.0.pickle\n",
      "INFO:blib2to3.pgen2.driver:Writing failed: [Errno 2] No such file or directory: '/home/joe/.cache/black/21.12b0/tmpp1c_tsoy'\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import actions\n",
    "import mcmc\n",
    "import transforms\n",
    "import utils\n",
    "\n",
    "Tensor: TypeAlias = torch.Tensor\n",
    "BoolTensor: TypeAlias = torch.BoolTensor\n",
    "Module: TypeAlias = torch.nn.Module\n",
    "IterableDataset: TypeAlias = torch.utils.data.IterableDataset\n",
    "\n",
    "PI = math.pi\n",
    "\n",
    "%load_ext lab_black\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5bbf22c-7c28-4836-a72b-4b9e79793a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingBlock(torch.nn.Module):\n",
    "    \"\"\"Pair of coupling layers.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform,\n",
    "        lattice_shape: list[int],\n",
    "        net_hidden_shape: list[int],\n",
    "        net_activation: Module,\n",
    "        net_final_activation: Module,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.register_buffer(\"mask\", utils.make_checkerboard(lattice_shape))\n",
    "\n",
    "        half_lattice = utils.prod(lattice_shape) // 2\n",
    "        nodes = [\n",
    "            half_lattice,\n",
    "            *net_hidden_shape,\n",
    "            half_lattice * self.transform.params_dof,\n",
    "        ]\n",
    "        activations = [net_activation for _ in net_hidden_shape] + [\n",
    "            net_final_activation\n",
    "        ]\n",
    "        net_a, net_b = [], []\n",
    "        for d_in, d_out, f_act in zip(nodes[:-1], nodes[1:], activations):\n",
    "            net_a.append(torch.nn.Linear(d_in, d_out))\n",
    "            net_b.append(torch.nn.Linear(d_in, d_out))\n",
    "            net_a.append(f_act)\n",
    "            net_b.append(f_act)\n",
    "        self.net_a = torch.nn.Sequential(*net_a)\n",
    "        self.net_b = torch.nn.Sequential(*net_b)\n",
    "\n",
    "    def forward(self, inputs: Tensor, log_det_jacob: Tensor) -> tuple[Tensor]:\n",
    "        in_a = inputs[:, self.mask]\n",
    "        in_b = inputs[:, ~self.mask]\n",
    "        out_a, log_det_jacob_a = self.transform(\n",
    "            in_a, self.net_b(in_b).view(*in_a.shape, -1).squeeze(dim=-1)\n",
    "        )\n",
    "        out_b, log_det_jacob_b = self.transform(\n",
    "            in_b, self.net_a(out_a).view(*in_b.shape, -1).squeeze(dim=-1)\n",
    "        )\n",
    "        log_det_jacob.add_(log_det_jacob_a + log_det_jacob_b)\n",
    "        outputs = torch.empty_like(inputs)\n",
    "        outputs[:, self.mask] = out_a\n",
    "        outputs[:, ~self.mask] = out_b\n",
    "        return outputs, log_det_jacob\n",
    "\n",
    "    def inverse(self, inputs: Tensor, log_det_jacob: Tensor) -> tuple[Tensor]:\n",
    "        in_a = inputs[:, self.mask]\n",
    "        in_b = inputs[:, ~self.mask]\n",
    "        out_b, log_det_jacob_b = self.transform.inverse(\n",
    "            in_b, self.net_a(in_a).view(*in_b.shape, -1).squeeze(dim=-1)\n",
    "        )\n",
    "        out_a, log_det_jacob_a = self.transform.inverse(\n",
    "            in_a, self.net_b(out_b).view(*in_a.shape, -1).squeeze(dim=-1)\n",
    "        )\n",
    "        log_det_jacob.add_(log_det_jacob_a)  # + log_det_jacob_b)\n",
    "        outputs = torch.empty_like(inputs)\n",
    "        outputs[:, self.mask] = out_a\n",
    "        outputs[:, ~self.mask] = out_b\n",
    "        return outputs, log_det_jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cbb5e45-4dad-49e5-99a4-cfcc0d5fe2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingBlockHalfConditional(torch.nn.Module):\n",
    "    \"\"\"Pair of coupling layers.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transform,\n",
    "        lattice_shape: list[int],\n",
    "        net_hidden_shape: list[int],\n",
    "        net_activation: Module,\n",
    "        net_final_activation: Module,\n",
    "        coupling_strength: float,\n",
    "        final_layer: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.register_buffer(\"mask\", utils.make_checkerboard(lattice_shape))\n",
    "\n",
    "        half_lattice = utils.prod(lattice_shape) // 2\n",
    "        nodes = [\n",
    "            half_lattice,\n",
    "            *net_hidden_shape,\n",
    "            half_lattice * self.transform.params_dof,\n",
    "        ]\n",
    "        activations = [net_activation for _ in net_hidden_shape] + [\n",
    "            net_final_activation\n",
    "        ]\n",
    "        net = []\n",
    "        for d_in, d_out, f_act in zip(nodes[:-1], nodes[1:], activations):\n",
    "            net.append(torch.nn.Linear(d_in, d_out))\n",
    "            net.append(f_act)\n",
    "        self.net = torch.nn.Sequential(*net)\n",
    "\n",
    "        lattice_dim = len(lattice_shape)\n",
    "        self.register_buffer(\"kernel\", utils.nearest_neighbour_kernel(lattice_dim))\n",
    "        if lattice_dim == 1:\n",
    "            self.conv = F.conv1d\n",
    "        elif lattice_dim == 2:\n",
    "            self.conv = F.conv2d\n",
    "        elif lattice_dim == 3:\n",
    "            self.conv = F.conv3d\n",
    "\n",
    "        padding = tuple(1 for edge in range(2 * lattice_dim))\n",
    "        self.pad = lambda config: F.pad(config, padding, \"circular\")\n",
    "\n",
    "        self.coupling_strength = coupling_strength\n",
    "        self.final_layer = final_layer\n",
    "\n",
    "    def sample_conditional(self, config):\n",
    "        config.unsqueeze_(dim=1)\n",
    "        cos_config, sin_config = config.cos(), config.sin()\n",
    "        m1 = self.conv(self.pad(cos_config), self.kernel).squeeze(dim=1)\n",
    "        m2 = self.conv(self.pad(sin_config), self.kernel).squeeze(dim=1)\n",
    "\n",
    "        m1 = m1[..., self.mask]\n",
    "        m2 = m2[..., self.mask]\n",
    "        kappa = self.coupling_strength * (m1.pow(2) + m2.pow(2)).sqrt()\n",
    "        kappa.clamp_(min=0.01)  # otherwise sampling takes AGES\n",
    "        theta = torch.atan2(m2, m1)\n",
    "\n",
    "        dist = torch.distributions.VonMises(loc=theta, concentration=kappa)\n",
    "        new_spins = dist.sample()\n",
    "        log_density = dist.log_prob(new_spins).sum(dim=1)\n",
    "        return new_spins.squeeze(dim=1), log_density\n",
    "\n",
    "    def forward(self, inputs: Tensor, log_det_jacob: Tensor) -> tuple[Tensor]:\n",
    "        in_a = inputs[:, self.mask]\n",
    "        in_b = inputs[:, ~self.mask]\n",
    "        out_a, log_det_jacob_a = self.transform(\n",
    "            in_a, self.net(in_b).view(*in_a.shape, -1).squeeze(dim=-1)\n",
    "        )\n",
    "        log_det_jacob.add_(log_det_jacob_a)\n",
    "\n",
    "        intermediates = torch.clone(inputs)\n",
    "        intermediates[:, self.mask] = out_a\n",
    "\n",
    "        outputs = torch.clone(intermediates)\n",
    "        out_b, log_density_b = self.sample_conditional(intermediates)\n",
    "        outputs[:, ~self.mask] = out_b\n",
    "\n",
    "        if self.final_layer:\n",
    "            log_det_jacob.add_(log_density_b)\n",
    "\n",
    "        return outputs, log_det_jacob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac0bafc-1e5e-4519-9d83-96272351f9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    \"\"\"Module which learns to sample from XY model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        xy_coupling: float,\n",
    "        lattice_shape: list[int],\n",
    "        n_blocks: int,\n",
    "        conditional_blocks: list[int],\n",
    "        n_spline_segments: int,\n",
    "        net_hidden_shape: list[int],\n",
    "        net_activation: torch.nn.Module,\n",
    "        use_shift_coupling_layers: bool = False,\n",
    "        use_random_rotations: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(n_blocks):\n",
    "            if use_shift_coupling_layers:\n",
    "                layers.append(\n",
    "                    CouplingBlock(\n",
    "                        transforms.PointwisePhaseShift(),\n",
    "                        lattice_shape,\n",
    "                        net_hidden_shape,\n",
    "                        net_activation=torch.nn.Tanh(),\n",
    "                        net_final_activation=torch.nn.Hardtanh(-PI, PI),\n",
    "                    )\n",
    "                )\n",
    "            if use_random_rotations:\n",
    "                layers.append(utils.RandomRotationLayer())\n",
    "\n",
    "            if i in conditional_blocks:\n",
    "                layers.append(\n",
    "                    CouplingBlockHalfConditional(\n",
    "                        transforms.PointwiseRationalQuadraticSplineTransform(\n",
    "                            n_spline_segments\n",
    "                        ),\n",
    "                        lattice_shape,\n",
    "                        net_hidden_shape,\n",
    "                        net_activation,\n",
    "                        net_final_activation=torch.nn.Identity(),\n",
    "                        coupling_strength=xy_coupling,\n",
    "                        final_layer=(True if i == n_blocks - 1 else False),\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                layers.append(\n",
    "                    CouplingBlock(\n",
    "                        transforms.PointwiseRationalQuadraticSplineTransform(\n",
    "                            n_spline_segments\n",
    "                        ),\n",
    "                        lattice_shape,\n",
    "                        net_hidden_shape,\n",
    "                        net_activation,\n",
    "                        net_final_activation=torch.nn.Identity(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.flow = utils.Flow(*layers)\n",
    "        self.action = actions.XYSpinAction(xy_coupling)\n",
    "        self.curr_iter = 0\n",
    "\n",
    "    def _save_checkpoint(self):\n",
    "        \"\"\"Hack: should be done automatically by Lightning but I never progress\n",
    "        past epoch=0 and this is easier than fiddling with other things.\n",
    "        \"\"\"\n",
    "        self.trainer.save_checkpoint(\n",
    "            self.logger.log_dir + f\"/checkpoints/iter_{self.curr_iter}.ckpt\"\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"z ~ Unif -> x ~ XY\"\"\"\n",
    "        z, log_prob_z = batch\n",
    "        x, log_det_jacob = self.flow(z)\n",
    "        weights = log_prob_z - log_det_jacob + self.action(x)\n",
    "        return x, weights\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self._save_checkpoint()\n",
    "        # Dirty but more convenient than saving / loading config files atm\n",
    "        self.logger.log_hyperparams(self.hparams)\n",
    "\n",
    "    def on_train_end(self):\n",
    "        self._save_checkpoint()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, weights = self.forward(batch)\n",
    "        loss = weights.mean()\n",
    "        self.log(\"loss\", loss, logger=True)\n",
    "        self.lr_schedulers().step()  # must be called manually since lightning 1.3!!\n",
    "        self.curr_iter += 1\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, weights = self.forward(batch)\n",
    "        loss = weights.mean()\n",
    "        metrics = dict(\n",
    "            loss=loss,\n",
    "            acceptance=utils.metropolis_acceptance(weights),\n",
    "            mag_sq=utils.magnetisation_sq(x).mean(),\n",
    "        )\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.flow.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.trainer.max_steps\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, prior: IterableDataset, n_iter: int = 1):\n",
    "        x, weights = self.forward(next(prior))\n",
    "        for _ in range(n_iter - 1):\n",
    "            _x, _weights = self.forward(next(prior))\n",
    "            x = torch.cat((x, _x), dim=0)\n",
    "            weights = torch.cat((weights, _weights), dim=0)\n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f6ece-6843-4de5-beee-2039166492fa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a69c7149-36da-4c04-8a2c-92ee3c1d128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XY parameters\n",
    "LATTICE_SHAPE = [6]\n",
    "XY_COUPLING = 0.8\n",
    "\n",
    "# Model parameters\n",
    "N_BLOCKS = 4\n",
    "CONDITIONAL_BLOCKS = [0, 1, 2, 3]\n",
    "N_SPLINE_SEGMENTS = 8\n",
    "NET_HIDDEN_SHAPE = [32]\n",
    "NET_ACTIVATION = torch.nn.Tanh()\n",
    "USE_SHIFT_COUPLING_LAYERS = False\n",
    "USE_RANDOM_ROTATIONS = False\n",
    "\n",
    "# Training hyperparameters\n",
    "N_TRAIN = 5000\n",
    "N_BATCH = 1000\n",
    "N_BATCH_VAL = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e08ec10-2534-468a-8069-fa7567219e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "model = Model(\n",
    "    xy_coupling=XY_COUPLING,\n",
    "    lattice_shape=LATTICE_SHAPE,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    conditional_blocks=CONDITIONAL_BLOCKS,\n",
    "    n_spline_segments=N_SPLINE_SEGMENTS,\n",
    "    net_hidden_shape=NET_HIDDEN_SHAPE,\n",
    "    net_activation=NET_ACTIVATION,\n",
    "    use_shift_coupling_layers=USE_SHIFT_COUPLING_LAYERS,\n",
    "    use_random_rotations=USE_RANDOM_ROTATIONS,\n",
    ")\n",
    "\n",
    "# Could wrap in DataSet(IterableDataset, batch_size=None) but can't currently see benefit\n",
    "unif = torch.distributions.Uniform(-PI, PI)\n",
    "train_dataloader = utils.Prior(\n",
    "    distribution=unif, sample_shape=[N_BATCH, *LATTICE_SHAPE]\n",
    ")\n",
    "val_dataloader = utils.Prior(\n",
    "    distribution=unif, sample_shape=[N_BATCH_VAL, *LATTICE_SHAPE]\n",
    ")\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\n",
    "    save_dir=\"test\",\n",
    "    name=\"spins\",\n",
    ")\n",
    "pbar = utils.JlabProgBar()\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_steps=N_TRAIN,  # total number of training steps\n",
    "    val_check_interval=100,  # how often to run sampling\n",
    "    limit_val_batches=1,  # one batch for each val step\n",
    "    # logger=logger,\n",
    "    callbacks=[pbar, lr_monitor],\n",
    "    enable_checkpointing=False,  # manually saving checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f7ae823-2c7d-442b-b105-85455429e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | flow | Flow | 10.0 K\n",
      "------------------------------\n",
      "10.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.0 K    Total params\n",
      "0.040     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 5101it [02:17, 37.21it/s, loss=-6.68, v_num=4]             \n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a7b7635-7c95-4da5-bd22-2461fbad44d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-aef8af768eee44d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-aef8af768eee44d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85223c3-4d64-4c4a-988d-1151131b430e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
