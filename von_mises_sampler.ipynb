{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41abdcfa-d50c-4bd7-a8cc-cc94bd583fbd",
   "metadata": {},
   "source": [
    "# Normalising flow for the von Mises distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3208ea-fc63-4a89-8e1f-c6273754b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "import transforms\n",
    "import utils\n",
    "\n",
    "PI = math.pi\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 18})\n",
    "\n",
    "%load_ext lab_black\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e088014-9b2b-47fc-831d-507151630ccb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Fixed von Mises concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649618c8-05d0-41e9-b9e0-1239a350cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnconditionalLayer(torch.nn.Module):\n",
    "    \"\"\"Wraps around a transformation, adding a set of learnable parameters.\"\"\"\n",
    "\n",
    "    def __init__(self, transform):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        self.params = torch.nn.Parameter(transform.identity_params.view(1, 1, -1))\n",
    "\n",
    "    def forward(self, inputs, log_det_jacob):\n",
    "        outputs, log_det_jacob_this = self.transform(\n",
    "            inputs, self.params.expand(inputs.shape[0], 1, -1)\n",
    "        )\n",
    "        log_det_jacob.add_(log_det_jacob_this)\n",
    "        return outputs, log_det_jacob\n",
    "\n",
    "    def inverse(self, inputs, log_det_jacob):\n",
    "        outputs, log_det_jacob_this = self.transform.inverse(\n",
    "            inputs, self.params.expand(inputs.shape[0], 1, -1)\n",
    "        )\n",
    "        log_det_jacob.add_(log_det_jacob_this)\n",
    "        return outputs, log_det_jacob\n",
    "\n",
    "\n",
    "class FixedConcModel(pl.LightningModule):\n",
    "    \"\"\"Module which learns to transform uniform variates into von Mises variates.\"\"\"\n",
    "\n",
    "    def __init__(self, vonmises_conc, n_spline_segments):\n",
    "        super().__init__()\n",
    "        self.flow = utils.Flow(\n",
    "            UnconditionalLayer(\n",
    "                transforms.PointwiseRationalQuadraticSplineTransform(n_spline_segments)\n",
    "            )\n",
    "        )\n",
    "        self.target = torch.distributions.VonMises(loc=0, concentration=vonmises_conc)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        z, log_prob_z = batch  # shape (n_batch, 1)\n",
    "        x, log_det_jacob = self.flow(z)\n",
    "        log_prob_x = self.target.log_prob(x).view_as(log_det_jacob)  # flattened\n",
    "        weights = log_prob_z - log_det_jacob - log_prob_x\n",
    "        return x, weights\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        _, weights = self.forward(batch)\n",
    "        loss = weights.mean()\n",
    "        self.log(\"loss\", loss, logger=True)\n",
    "        self.lr_schedulers().step()  # must be called manually since lightning 1.3!!\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, weights = self.forward(batch)\n",
    "        loss = weights.mean()\n",
    "        self.log(\n",
    "            \"acceptance\",\n",
    "            utils.metropolis_acceptance(weights.flatten()),\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.flow.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=self.trainer.max_steps\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, prior, n_iter=1):\n",
    "        x, weights = self.forward(next(prior))\n",
    "        for _ in range(n_iter - 1):\n",
    "            _x, _weights = self.forward(next(prior))\n",
    "            x = torch.cat((x, _x), dim=0)\n",
    "            weights = torch.cat((weights, _weights), dim=0)\n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6264f187-4a45-4f34-b628-ad2ea79c58e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VONMISES_CONC = 0.5\n",
    "N_SPLINE_SEGMENTS = 8\n",
    "\n",
    "N_TRAIN = 10000\n",
    "N_BATCH = 1000\n",
    "N_BATCH_VAL = 10000\n",
    "\n",
    "model = FixedConcModel(VONMISES_CONC, N_SPLINE_SEGMENTS)\n",
    "\n",
    "unif = torch.distributions.Uniform(-PI, PI)\n",
    "train_dataloader = utils.Prior(distribution=unif, sample_shape=[N_BATCH, 1])\n",
    "val_dataloader = utils.Prior(distribution=unif, sample_shape=[N_BATCH_VAL, 1])\n",
    "\n",
    "pbar = utils.JlabProgBar()\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_steps=N_TRAIN,\n",
    "    val_check_interval=100,  # how often to run sampling\n",
    "    limit_val_batches=1,\n",
    "    callbacks=[pbar, lr_monitor],  # gpu_monitor],\n",
    "    enable_checkpointing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "97b6c9c1-8fd4-4712-952a-aa53fa9d1f66",
   "metadata": {
    "tags": []
   },
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16575543-02a4-4702-a028-cbbc5d559527",
   "metadata": {},
   "source": [
    "x, weights = model.sample(val_dataloader, n_iter=10)\n",
    "\n",
    "dom = torch.linspace(-math.pi, math.pi, 100)\n",
    "target = model.target.log_prob(dom).exp()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(x.flatten().tolist(), bins=100, density=True, label=\"model\")\n",
    "ax.plot(dom.tolist(), target.tolist(), \"r-\", label=\"target\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9edfe17-4455-481a-8da5-ad9053c26010",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Variable von Mises concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a515f6-78e3-428b-933a-8b54aa815de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalLayer(torch.nn.Module):\n",
    "    \"\"\"Wraps around a transformation, adding a neural network taking conc as input.\"\"\"\n",
    "\n",
    "    def __init__(self, transform, net_hidden_shape, net_activation):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        nodes = [1, *net_hidden_shape, transform.params_dof]\n",
    "        activations = [net_activation for _ in net_hidden_shape] + [torch.nn.Identity()]\n",
    "        layers = []\n",
    "        for d_in, d_out, f_act in zip(nodes[:-1], nodes[1:], activations):\n",
    "            layers.append(torch.nn.Linear(d_in, d_out))\n",
    "            layers.append(f_act)\n",
    "        self.network = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, inputs, log_det_jacob, vm_conc):\n",
    "        params = self.network(vm_conc)\n",
    "        outputs, log_det_jacob_this = self.transform(inputs, params.unsqueeze(dim=1))\n",
    "        log_det_jacob.add_(log_det_jacob_this)\n",
    "        return outputs, log_det_jacob\n",
    "\n",
    "    def inverse(self, inputs, log_det_jacob, vm_conc):\n",
    "        params = self.network(vm_conc)\n",
    "        outputs, log_det_jacob_this = self.transform.inverse(\n",
    "            inputs, params.unsqueeze(dim=1)\n",
    "        )\n",
    "        log_det_jacob.add_(log_det_jacob_this)\n",
    "        return outputs, log_det_jacob\n",
    "\n",
    "\n",
    "class VariableConcModel(pl.LightningModule):\n",
    "    \"\"\"Module which learns a mapping between uniform and von Mises distributions.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_concentration,\n",
    "        n_spline_segments,\n",
    "        net_hidden_shape,\n",
    "        net_activation,\n",
    "        use_fwd_kl=False,\n",
    "        fwd_kl_interval=1,\n",
    "        use_mobius=False,\n",
    "        concentration_range=[0.02, 11],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # NOTE: sampling from torch.distributions.VonMises fails at conc=1e-4!!!\n",
    "        self.concentration_distribution = torch.distributions.Uniform(\n",
    "            *concentration_range\n",
    "        )\n",
    "        self.curr_iter = 1\n",
    "\n",
    "        spline_layer = ConditionalLayer(\n",
    "            transforms.PointwiseRationalQuadraticSplineTransform(n_spline_segments),\n",
    "            net_hidden_shape,\n",
    "            net_activation,\n",
    "        )\n",
    "\n",
    "        if use_mobius:\n",
    "            self.flow = utils.Flow(utils.MobiusLayer(), spline_layer)\n",
    "        else:\n",
    "            self.flow = utils.Flow(spline_layer)\n",
    "\n",
    "    def _uniform_to_vonmises(self, z, log_prob_z, concentrations):\n",
    "        \"\"\"z ~ Unif -> x ~ vM\"\"\"\n",
    "        x, log_det_jacob = self.flow(z, concentrations)\n",
    "        log_prob_x = (\n",
    "            torch.distributions.VonMises(loc=0, concentration=concentrations)\n",
    "            .log_prob(x)\n",
    "            .view_as(log_det_jacob)\n",
    "        )\n",
    "        weights = log_prob_z - log_det_jacob - log_prob_x\n",
    "        return x, weights\n",
    "\n",
    "    def _vonmises_to_uniform(self, x, log_prob_x, concentrations):\n",
    "        \"\"\"x ~ vM -> z ~ Unif\"\"\"\n",
    "        z, log_det_jacob = self.flow.inverse(x, concentrations)\n",
    "        weights = log_prob_x - log_det_jacob + math.log(2 * PI)\n",
    "        return z, weights\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.logger.log_hyperparams(self.hparams)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        z, log_prob_z = batch\n",
    "        concentrations = self.concentration_distribution.sample(z.shape).to(z.device)\n",
    "\n",
    "        if (\n",
    "            self.hparams.use_fwd_kl\n",
    "            and self.curr_iter % self.hparams.fwd_kl_interval == 0\n",
    "        ):\n",
    "            curr_vonmises_prior = torch.distributions.VonMises(\n",
    "                loc=0, concentration=concentrations\n",
    "            )\n",
    "            x = curr_vonmises_prior.sample()\n",
    "            log_prob_x = curr_vonmises_prior.log_prob(x).flatten()\n",
    "            _, weights = self._vonmises_to_uniform(x, log_prob_x, concentrations)\n",
    "        else:\n",
    "            _, weights = self._uniform_to_vonmises(z, log_prob_z, concentrations)\n",
    "\n",
    "        loss = weights.mean()\n",
    "        self.log(\"loss\", loss, logger=True)\n",
    "        self.lr_schedulers().step()  # must be called manually since lightning 1.3!!\n",
    "        self.curr_iter += 1\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        z, log_prob_z = batch\n",
    "        concentrations = self.concentration_distribution.sample(z.shape).to(z.device)\n",
    "        x, weights = self._uniform_to_vonmises(z, log_prob_z, concentrations)\n",
    "        loss = weights.mean()\n",
    "        self.log(\n",
    "            \"acceptance\",\n",
    "            utils.metropolis_acceptance(weights.flatten()),\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.flow.parameters(), lr=0.001)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=self.trainer.max_steps,\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, concentration, prior, n_iter=1):\n",
    "        concentrations = torch.full(prior.sample_shape, concentration)\n",
    "        x, weights = self._uniform_to_vonmises(*next(prior), concentrations)\n",
    "        for _ in range(n_iter - 1):\n",
    "            _x, _weights = self._uniform_to_vonmises(*next(prior), concentrations)\n",
    "            x = torch.cat((x, _x), dim=0)\n",
    "            weights = torch.cat((weights, _weights), dim=0)\n",
    "        return x, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575eef45-66ad-4b6e-a8bf-0e2d08be6e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "VONMISES_MAX_CONC = 10\n",
    "N_SPLINE_SEGMENTS = 16\n",
    "NET_HIDDEN_SHAPE = [8, 16, 32]\n",
    "NET_ACTIVATION = torch.nn.LeakyReLU()\n",
    "USE_FWD_KL = False\n",
    "FWD_KL_INTERVAL = 2\n",
    "USE_MOBIUS = True\n",
    "\n",
    "N_TRAIN = 32000\n",
    "N_BATCH = 8000\n",
    "N_BATCH_VAL = 16000\n",
    "\n",
    "model = VariableConcModel(\n",
    "    VONMISES_MAX_CONC,\n",
    "    N_SPLINE_SEGMENTS,\n",
    "    NET_HIDDEN_SHAPE,\n",
    "    NET_ACTIVATION,\n",
    "    USE_FWD_KL,\n",
    "    FWD_KL_INTERVAL,\n",
    "    USE_MOBIUS,\n",
    ")\n",
    "\n",
    "unif = torch.distributions.Uniform(-PI, PI)\n",
    "train_dataloader = utils.Prior(distribution=unif, sample_shape=[N_BATCH, 1])\n",
    "val_dataloader = utils.Prior(distribution=unif, sample_shape=[N_BATCH_VAL, 1])\n",
    "\n",
    "pbar = utils.JlabProgBar()\n",
    "lr_monitor = pl.callbacks.LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_steps=N_TRAIN,\n",
    "    val_check_interval=100,  # how often to run sampling\n",
    "    limit_val_batches=1,\n",
    "    callbacks=[pbar, lr_monitor],  # gpu_monitor],\n",
    "    enable_checkpointing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7c266-55f8-45a1-9ebb-165447b0a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef9e44-b8a1-4542-8333-f71f4c4f3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CONC = 12\n",
    "N_COLS = 4\n",
    "\n",
    "concentrations = torch.linspace(\n",
    "    model.concentration_distribution.low,\n",
    "    model.concentration_distribution.high,\n",
    "    N_CONC,\n",
    ").tolist()\n",
    "dom = torch.linspace(-math.pi, math.pi, 100)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    math.ceil(N_CONC / N_COLS),\n",
    "    N_COLS,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    figsize=(20, 16),\n",
    ")\n",
    "\n",
    "acceptances = []\n",
    "\n",
    "for ax, conc in zip(axes.flatten(), concentrations):\n",
    "    x, weights = model.sample(conc, val_dataloader, n_iter=10)\n",
    "\n",
    "    acceptance = utils.metropolis_acceptance(weights.flatten())\n",
    "    acceptances.append(acceptance)\n",
    "\n",
    "    target = torch.distributions.VonMises(loc=0, concentration=conc).log_prob(dom).exp()\n",
    "\n",
    "    ax.hist(x.flatten().tolist(), bins=50, density=True)\n",
    "    ax.plot(dom.tolist(), target.tolist(), \"r-\", label=f\"conc = {conc:.2g}\")\n",
    "    ax.legend()\n",
    "\n",
    "fig.suptitle(\"Histograms of model outputs\")\n",
    "\n",
    "acceptance_fig, acceptance_ax = plt.subplots()\n",
    "acceptance_ax.plot(concentrations, acceptances, \"o-\")\n",
    "acceptance_ax.set_xlabel(\"concentration\")\n",
    "acceptance_ax.set_ylabel(\"acceptance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792034ad-f276-40e5-b7ed-14ba98cbb728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5c191-1c2e-418e-afe9-1d97a4a93990",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"trained_vonmises_sampler/model.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
